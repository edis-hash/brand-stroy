import praw
import google.generativeai as genai
import os

# ================= é…ç½®åŒºåŸŸ (è¯·å¡«å…¥ä½ çš„ Key) =================
# 1. Google Gemini API è®¾ç½® (https://aistudio.google.com/app/apikey)
GOOGLE_API_KEY = "ä½ çš„_GOOGLE_API_KEY"  # æ›¿æ¢è¿™é‡Œ

# 2. Reddit API è®¾ç½® (https://www.reddit.com/prefs/apps)
REDDIT_CLIENT_ID = "ä½ çš„_REDDIT_CLIENT_ID"     # æ›¿æ¢è¿™é‡Œ
REDDIT_CLIENT_SECRET = "ä½ çš„_REDDIT_SECRET"    # æ›¿æ¢è¿™é‡Œ
REDDIT_USER_AGENT = "AmazonTrendBot/2.0 (Gemini)"

# ==========================================================

class AmazonTrendAgent:
    def __init__(self):
        # é…ç½® Gemini
        genai.configure(api_key=GOOGLE_API_KEY)
        # ä½¿ç”¨ gemini-1.5-flashï¼Œé€Ÿåº¦å¿«ä¸”å…è´¹é¢åº¦é«˜ï¼Œä¸Šä¸‹æ–‡æå…¶å·¨å¤§
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
        # é…ç½® Reddit
        self.reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT
        )
        print("ğŸ¤– Google Gemini æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ...")

    def translate_to_english(self, chinese_keyword):
        """Step 1: å°†ä¸­æ–‡è¾“å…¥è½¬åŒ–ä¸ºè‹±æ–‡æœç´¢è¯"""
        print(f"ğŸ”„ æ­£åœ¨è°ƒç”¨ Gemini ç¿»è¯‘å…³é”®è¯: {chinese_keyword}...")
        
        prompt = f"You are a translator. Translate '{chinese_keyword}' to a single English keyword or phrase suitable for Reddit search. Only output the English text, nothing else."
        
        response = self.model.generate_content(prompt)
        eng_keyword = response.text.strip()
        print(f"âœ… è‹±æ–‡æœç´¢è¯: {eng_keyword}")
        return eng_keyword

    def scrape_reddit_data(self, keyword, limit=50):
        """Step 2: æŠ“å– Reddit æ•°æ®"""
        print(f"ğŸ•µï¸ æ­£åœ¨ Reddit ä¸ŠæŒ–æ˜ '{keyword}' çš„ç›¸å…³æ•°æ® (Top {limit} posts)...")
        
        # æ‰©å¤§æœç´¢èŒƒå›´ï¼Œå› ä¸º Gemini åƒå¾—ä¸‹ï¼
        target_subreddits = "all+NFL+HomeDecorating+Plushies+Gifts+PartyPlanning+DIY"
        subreddit = self.reddit.subreddit(target_subreddits)
        
        posts_data = []
        try:
            for submission in subreddit.search(keyword, sort='relevance', time_filter='month', limit=limit):
                # æ„å»ºç»“æ„åŒ–æ•°æ®
                post_content = f"--- POST START ---\nTitle: {submission.title}\nSubreddit: {submission.subreddit.display_name}\nScore: {submission.score}"
                
                # è·å–æ›´å¤šè¯„è®ºï¼Œå› ä¸º Gemini ä¸Šä¸‹æ–‡å¾ˆå¤§ï¼Œæˆ‘ä»¬å¯ä»¥å¤šæ‹¿ä¸€ç‚¹
                submission.comments.replace_more(limit=0)
                top_comments = ""
                for comment in submission.comments.list()[:5]: # å¢åŠ åˆ°å‰5æ¡è¯„è®º
                    top_comments += f"\n- Comment (Score {comment.score}): {comment.body}"
                
                full_text = post_content + "\nTop Comments:" + top_comments + "\n--- POST END ---\n"
                posts_data.append(full_text)
        except Exception as e:
            print(f"âš ï¸ æŠ“å–è¿‡ç¨‹ä¸­å‡ºç°å°é”™è¯¯ (å¯å¿½ç•¥): {e}")

        print(f"ğŸ“¦ æˆåŠŸæŠ“å– {len(posts_data)} æ¡è®¨è®ºå¸–ï¼Œå‡†å¤‡æŠ•å–‚ç»™ Geminiã€‚")
        return "\n".join(posts_data)

    def analyze_trends(self, data_text, original_topic):
        """Step 3: ä½¿ç”¨ Gemini è¿›è¡Œé€‰å“åˆ†æ"""
        print("ğŸ§  æ­£åœ¨è°ƒç”¨ Gemini 1.5 Flash è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        # Gemini 1.5 Flash æ‹¥æœ‰ 100ä¸‡ token ä¸Šä¸‹æ–‡ï¼Œä¸éœ€è¦åƒ OpenAI é‚£æ ·æˆªæ–­æ•°æ®
        # æˆ‘ä»¬å¯ä»¥ç›´æ¥æŠŠæ•´ä¸ªå·¨å¤§çš„æ–‡æœ¬æ‰”è¿›å»
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äºšé©¬é€Šé€‰å“ä¸“å®¶ã€‚ä½ è¦ä¸ºä¸»é¢˜â€œ{original_topic}â€åˆ†æä»¥ä¸‹æ¥è‡ª Reddit çš„åŸå§‹è®¨è®ºæ•°æ®ã€‚
        
        è¯·è¾“å‡ºä¸€ä»½ç»“æ„æ¸…æ™°çš„ã€äºšé©¬é€Šé€‰å“æ´å¯ŸæŠ¥å‘Šã€‘ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆè¯·ç”¨ä¸­æ–‡å›ç­”ï¼‰ï¼š
        
        1. **ğŸ”¥ é£™å‡å…³é”®è¯ (Top 5 Keywords)**: 
           - é‡ç‚¹æŒ–æ˜å½¢å®¹è¯+åè¯çš„ç»„åˆï¼ˆä¾‹å¦‚ "Sparkly Helmet", "Weighted Plush"ï¼‰ã€‚
           - æ’é™¤åƒ "Super Bowl" è¿™ç§è¿‡äºå®½æ³›çš„å¤§è¯ï¼Œæ‰¾å…·ä½“çš„é•¿å°¾è¯/äº§å“å±æ€§è¯ã€‚
           
        2. **ğŸ˜« ç”¨æˆ·ç—›ç‚¹æ·±åº¦æŒ–æ˜ (Pain Points)**:
           - ç”¨æˆ·åœ¨æŠ±æ€¨ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šâ€œæ‰¾ä¸åˆ°è¿™ä¸ªé¢œè‰²çš„è£…é¥°â€ï¼Œâ€œå…¬ä»”å¤ªç¡¬äº†â€ï¼Œâ€œé€ç¤¼æ²¡æ–°æ„â€ï¼‰ã€‚
           - å¼•ç”¨ä¸€ä¸¤ä¸ªå…·ä½“çš„è¯„è®ºå†…å®¹ä½è¯ã€‚
           
        3. **ğŸ’¡ è½åœ°é€‰å“å»ºè®® (Actionable Products)**:
           - **é’ˆå¯¹ã€è£…é¥°å“ã€‘ç±»ç›®**æ¨è 1 ä¸ªå…·ä½“äº§å“æ–¹å‘ï¼ˆåŒ…å«æè´¨ã€é¢œè‰²ã€é£æ ¼å»ºè®®ï¼‰ã€‚
           - **é’ˆå¯¹ã€æ¯›ç»’ç©å…·ã€‘ç±»ç›®**æ¨è 1 ä¸ªå…·ä½“äº§å“æ–¹å‘ï¼ˆåŒ…å«åŠŸèƒ½ã€è§¦æ„Ÿã€äººç¾¤å»ºè®®ï¼‰ã€‚
           - *å¿…é¡»è¯´æ˜è¯¥äº§å“è§£å†³äº†ä»€ä¹ˆç—›ç‚¹ã€‚*

        4. **ğŸ” æµé‡éªŒè¯æŒ‡ä»¤**:
           - ç»™å‡º 3 ä¸ªè‹±æ–‡æœç´¢è¯ï¼Œä»¥ä¾¿æˆ‘å» Google Trends æˆ– Amazon ABA éªŒè¯ã€‚

        ä»¥ä¸‹æ˜¯ Reddit æ•°æ®ï¼š
        {data_text}
        """

        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"âŒ åˆ†æå¤±è´¥: {e}"

    def run(self, topic):
        print("="*50)
        print(f"ğŸš€ Amazon Trend Agent (Geminiç‰ˆ) å¯åŠ¨ - ç›®æ ‡: {topic}")
        print("="*50)
        
        # 1. ç¿»è¯‘
        eng_keyword = self.translate_to_english(topic)
        
        # 2. æŠ“å–
        raw_data = self.scrape_reddit_data(eng_keyword, limit=60) # ç¨å¾®å¢åŠ æŠ“å–é‡
        
        if not raw_data:
            print("âŒ æœªæ‰¾åˆ°è¶³å¤Ÿçš„æ•°æ®ã€‚")
            return

        # 3. åˆ†æ
        report = self.analyze_trends(raw_data, topic)
        
        print("\n" + "="*20 + " ğŸ“Š Gemini é€‰å“æŠ¥å‘Š " + "="*20 + "\n")
        print(report)
        print("\n" + "="*60)
        print("âœ… SOP æ‰§è¡Œå®Œæ¯•ã€‚ç”± Google Gemini é©±åŠ¨ã€‚")

# ================= æ‰§è¡Œå…¥å£ =================
if __name__ == "__main__":
    agent = AmazonTrendAgent()
    user_input = input("è¯·è¾“å…¥ä½ æƒ³æŒ–æ˜çš„ä¸»é¢˜ (ä¸­æ–‡ï¼Œä¾‹å¦‚: è¶…çº§ç¢— / æƒ…äººèŠ‚ / éœ²è¥): ")
    if user_input:
        agent.run(user_input)

import streamlit as st
import praw
import google.generativeai as genai
import time

# ================= é…ç½®åŒºåŸŸ (è¯·å¡«å…¥ä½ çš„ Key) =================
# ã€å®‰å…¨æç¤ºã€‘åœ¨æ­£å¼éƒ¨ç½²æ—¶ï¼Œå»ºè®®ä½¿ç”¨ st.secrets æ¥ç®¡ç†è¿™äº›æ•æ„Ÿä¿¡æ¯ï¼Œä¸è¦ç›´æ¥å†™åœ¨ä»£ç é‡Œã€‚
GOOGLE_API_KEY = "ä½ çš„_GOOGLE_API_KEY"  # æ›¿æ¢è¿™é‡Œ
REDDIT_CLIENT_ID = "ä½ çš„_REDDIT_CLIENT_ID"     # æ›¿æ¢è¿™é‡Œ
REDDIT_CLIENT_SECRET = "ä½ çš„_REDDIT_SECRET"    # æ›¿æ¢è¿™é‡Œ
REDDIT_USER_AGENT = "AmazonTrendBot/StreamlitUI/1.0"
# ==========================================================

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="äºšé©¬é€Šé€‰å“è¶‹åŠ¿æ™ºèƒ½ä½“",
    page_icon="ğŸ›ï¸",
    layout="centered"
)

# ç¼“å­˜èµ„æºï¼šç¡®ä¿ Reddit å’Œ Gemini å®¢æˆ·ç«¯åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œæé«˜è¿è¡Œæ•ˆç‡
@st.cache_resource
def get_agents():
    # é…ç½® Gemini
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    # é…ç½® Reddit
    reddit = praw.Reddit(
        client_id=REDDIT_CLIENT_ID,
        client_secret=RED_CLIENT_SECRET,
        user_agent=REDDIT_USER_AGENT
    )
    return model, reddit

# è·å–åˆå§‹åŒ–çš„æ™ºèƒ½ä½“
try:
    model_agent, reddit_agent = get_agents()
    st.sidebar.success("API ä»£ç†è¿æ¥æˆåŠŸ!")
except Exception as e:
    st.sidebar.error(f"API è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Keys: {e}")
    st.stop()


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (ä»ä¹‹å‰çš„ Class æ”¹å†™ä¸ºç‹¬ç«‹å‡½æ•°) ---

def translate_to_english(chinese_keyword):
    """Step 1: ä½¿ç”¨ Gemini ç¿»è¯‘"""
    prompt = f"You are a translator. Translate '{chinese_keyword}' to a single English keyword or phrase suitable for Reddit search. Only output the English text, nothing else."
    response = model_agent.generate_content(prompt)
    return response.text.strip()

def scrape_reddit_data(keyword, limit=60):
    """Step 2: æŠ“å– Reddit æ•°æ®"""
    # æ‰©å¤§æœç´¢èŒƒå›´
    target_subreddits = "all+NFL+HomeDecorating+Plushies+Gifts+PartyPlanning+DIY+Toys"
    subreddit = reddit_agent.subreddit(target_subreddits)
    
    posts_data = []
    try:
        # ä½¿ç”¨ st.status æ˜¾ç¤ºæŠ“å–è¿›åº¦ (Streamlit ç‰¹æ€§)
        with st.status(f"æ­£åœ¨ Reddit ä¸ŠæŒ–æ˜ '{keyword}'...", expanded=True) as status:
            count = 0
            st.write("è¿æ¥ Reddit API...")
            search_results = subreddit.search(keyword, sort='relevance', time_filter='month', limit=limit)
            
            st.write("å¼€å§‹æŠ“å–å¸–å­å’Œè¯„è®º...")
            for submission in search_results:
                count += 1
                # æ„å»ºç»“æ„åŒ–æ•°æ®
                post_content = f"--- POST START ---\nTitle: {submission.title}\nSubreddit: {submission.subreddit.display_name}\nScore: {submission.score}"
                
                submission.comments.replace_more(limit=0)
                top_comments = ""
                # æŠ“å–å‰ 5 æ¡é«˜èµè¯„è®º
                for comment in submission.comments.list()[:5]: 
                    top_comments += f"\n- Comment (Score {comment.score}): {comment.body}"
                
                full_text = post_content + "\nTop Comments:" + top_comments + "\n--- POST END ---\n"
                posts_data.append(full_text)
                
                if count % 10 == 0:
                    st.write(f"å·²æŠ“å– {count} æ¡å¸–å­...")
                    
            status.update(label=f"æŠ“å–å®Œæˆï¼å…± {len(posts_data)} æ¡æœ‰æ•ˆæ•°æ®ã€‚", state="complete", expanded=False)
            
    except Exception as e:
        st.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None

    return "\n".join(posts_data) if posts_data else None

def analyze_trends(data_text, original_topic):
    """Step 3: ä½¿ç”¨ Gemini è¿›è¡Œé€‰å“åˆ†æ"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äºšé©¬é€Šé€‰å“ä¸“å®¶ã€‚ä½ è¦ä¸ºä¸»é¢˜â€œ{original_topic}â€åˆ†æä»¥ä¸‹æ¥è‡ª Reddit çš„åŸå§‹è®¨è®ºæ•°æ®ã€‚
    
    è¯·è¾“å‡ºä¸€ä»½ç»“æ„æ¸…æ™°çš„ã€äºšé©¬é€Šé€‰å“æ´å¯ŸæŠ¥å‘Šã€‘ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆè¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨ Markdown æ ¼å¼ï¼‰ï¼š
    
    ### 1. ğŸ”¥ é£™å‡å…³é”®è¯ (Top 5 Keywords)
    - é‡ç‚¹æŒ–æ˜å½¢å®¹è¯+åè¯çš„ç»„åˆï¼ˆä¾‹å¦‚ "Sparkly Helmet", "Weighted Plush"ï¼‰ã€‚
    - æ’é™¤åƒ "{original_topic}" è¿™ç§è¿‡äºå®½æ³›çš„å¤§è¯ï¼Œæ‰¾å…·ä½“çš„é•¿å°¾è¯/äº§å“å±æ€§è¯ã€‚
       
    ### 2. ğŸ˜« ç”¨æˆ·ç—›ç‚¹æ·±åº¦æŒ–æ˜ (Pain Points)
    - ç”¨æˆ·åœ¨æŠ±æ€¨ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šâ€œæ‰¾ä¸åˆ°è¿™ä¸ªé¢œè‰²çš„è£…é¥°â€ï¼Œâ€œå…¬ä»”å¤ªç¡¬äº†â€ï¼Œâ€œé€ç¤¼æ²¡æ–°æ„â€ï¼‰ã€‚
    - *è¯·åŠ¡å¿…å¼•ç”¨ä¸€ä¸¤ä¸ªå…·ä½“çš„è¯„è®ºå†…å®¹ä½è¯ã€‚*
       
    ### 3. ğŸ’¡ è½åœ°é€‰å“å»ºè®® (Actionable Products)
    - **é’ˆå¯¹ã€è£…é¥°å“ã€‘ç±»ç›®**æ¨è 1 ä¸ªå…·ä½“äº§å“æ–¹å‘ï¼ˆåŒ…å«æè´¨ã€é¢œè‰²ã€é£æ ¼å»ºè®®ï¼‰ã€‚
    - **é’ˆå¯¹ã€æ¯›ç»’ç©å…·/ç¤¼ç‰©ã€‘ç±»ç›®**æ¨è 1 ä¸ªå…·ä½“äº§å“æ–¹å‘ï¼ˆåŒ…å«åŠŸèƒ½ã€è§¦æ„Ÿã€äººç¾¤å»ºè®®ï¼‰ã€‚
    - *å¿…é¡»è¯´æ˜è¯¥äº§å“è§£å†³äº†ä»€ä¹ˆç—›ç‚¹ã€‚*

    ### 4. ğŸ” æµé‡éªŒè¯æŒ‡ä»¤
    - ç»™å‡º 3 ä¸ªè‹±æ–‡æœç´¢è¯ï¼Œä»¥ä¾¿æˆ‘å» Google Trends æˆ– Amazon ABA éªŒè¯ã€‚

    ä»¥ä¸‹æ˜¯ Reddit æ•°æ®ï¼š
    {data_text}
    """
    try:
        response = model_agent.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"âŒ åˆ†æå¤±è´¥: {e}"

# ================= UI ç•Œé¢æ„å»º =================

st.title("ğŸ›ï¸ äºšé©¬é€Šé€‰å“è¶‹åŠ¿æ™ºèƒ½ä½“")
st.markdown("åŸºäº **Reddit èˆ†æƒ…** + **Google Gemini** å¤§æ¨¡å‹çš„è‡ªåŠ¨åŒ– SOP æŒ–æ˜å·¥å…·ã€‚")

# ä¾§è¾¹æ è¯´æ˜
with st.sidebar:
    st.header("ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. åœ¨å³ä¾§è¾“å…¥æ¡†è¾“å…¥ä½ æƒ³æŒ–æ˜çš„ä¸»é¢˜ï¼ˆä¸­æ–‡å³å¯ï¼‰ã€‚
    2. ç‚¹å‡»â€œå¼€å§‹å…¨æµç¨‹æŒ–æ˜â€æŒ‰é’®ã€‚
    3. ç³»ç»Ÿå°†è‡ªåŠ¨æ‰§è¡Œï¼šç¿»è¯‘ -> æŠ“å– Reddit æ•°æ® -> AI æ·±åº¦åˆ†æã€‚
    4. è€å¿ƒç­‰å¾…æŠ¥å‘Šç”Ÿæˆã€‚
    """)
    st.info("ç”± gemini-1.5-flash æ¨¡å‹é©±åŠ¨")

# ä¸»è¦è¾“å…¥åŒº
topic_input = st.text_input("è¯·è¾“å…¥ä¸»é¢˜å…³é”®è¯ (ä¾‹å¦‚: è¶…çº§ç¢— / ä¸‡åœ£èŠ‚ / éœ²è¥)", placeholder="åœ¨è¿™é‡Œè¾“å…¥...")
run_button = st.button("ğŸš€ å¼€å§‹å…¨æµç¨‹æŒ–æ˜", type="primary")

st.divider()

# ä¸»é€»è¾‘åŒºåŸŸ
if run_button:
    if not topic_input.strip():
        st.warning("è¯·å…ˆè¾“å…¥ä¸€ä¸ªä¸»é¢˜å…³é”®è¯ï¼")
    else:
        # 1. ç¿»è¯‘é˜¶æ®µ
        with st.spinner("æ­£åœ¨æ€è€ƒå¹¶ç¿»è¯‘å…³é”®è¯..."):
            eng_keyword = translate_to_english(topic_input)
        st.success(f"âœ… è‹±æ–‡æœç´¢è¯å·²ç¡®è®¤ä¸º: **{eng_keyword}**")
        
        # 2. æŠ“å–é˜¶æ®µ (å‡½æ•°å†…éƒ¨åŒ…å«äº† st.status æ˜¾ç¤ºè¿›åº¦)
        raw_data = scrape_reddit_data(eng_keyword, limit=70)
        
        if not raw_data:
            st.error("âŒ æœªèƒ½æ‰¾åˆ°è¶³å¤Ÿçš„æ•°æ®ã€‚è¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚")
        else:
            # 3. åˆ†æé˜¶æ®µ
            with st.spinner("ğŸ§  Gemini å¤§è„‘æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ (è¿™å¯èƒ½éœ€è¦ 30-60 ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…)..."):
                # ç¨å¾®æ¨¡æ‹Ÿä¸€ç‚¹å»¶è¿Ÿæ„Ÿ
                time.sleep(1)
                report = analyze_trends(raw_data, topic_input)
            
            # ç»“æœå±•ç¤º
            st.balloons() # æˆåŠŸåŠ¨ç”»æ•ˆæœ
            st.subheader(f"ğŸ“Š å…³äºâ€œ{topic_input}â€çš„é€‰å“æ´å¯ŸæŠ¥å‘Š")
            st.markdown(report)
            st.success("SOP æ‰§è¡Œå®Œæ¯•ï¼è¯·å‚è€ƒä¸Šè¿°å…³é”®è¯è¿›è¡Œ ABA éªŒè¯ã€‚")
